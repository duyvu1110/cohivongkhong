{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/qtxu/Sentiment-SPN/data/Ele-COQE/train.txt\"\n",
    "file_write = \"/home/qtxu/Sentiment-SPN/data/Ele-COQE/train_sentences_labels.txt\"\n",
    "# Open the file\n",
    "with open(file, \"r\") as fr, open(file_write, \"w\") as fw:\n",
    "  # Read the lines in the file\n",
    "  lines = fr.readlines()\n",
    "\n",
    "  for line in lines:\n",
    "        try:\n",
    "            sentence, label = line.strip().split(\"\\t\")\n",
    "            fw.write(sentence+\"\\t\"+label+\"\\n\")\n",
    "            # fw.write(sentence+\"\\n\") # consider comparative sentence and non somparative\n",
    "        except:\n",
    "            assert \"check again!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split()\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"/home/qtxu/PLM/bert-base-uncased\")\n",
    "\n",
    "def dependency_adj_matrix(text):\n",
    "    # https://spacy.io/docs/usage/processing-text\n",
    "    tokens = nlp(text)\n",
    "    tokenized = tokenizer(text.split(\" \"), is_split_into_words=True, add_special_tokens=False)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    words = text.split()\n",
    "    # matrix = np.zeros((len(words), len(words))).astype('float32')\n",
    "    matrix1 = np.zeros((len(word_ids), len(word_ids))).astype('float32')\n",
    "    assert len(words) == len(list(tokens))\n",
    "    assert (len(tokens) - 1) == max(word_ids)\n",
    "\n",
    "    for i, idx in enumerate(word_ids):\n",
    "        matrix1[i][i] = 1\n",
    "        for j, id in enumerate(word_ids):\n",
    "            if tokens[id] in tokens[idx].children or word_ids[j] == word_ids[i]:\n",
    "                matrix1[i][j] = 1\n",
    "                matrix1[j][i] = 1\n",
    "    return matrix1\n",
    "\n",
    "def softmax(x):\n",
    "    if len(x.shape) > 1:\n",
    "        # matrix\n",
    "        tmp = np.max(x, axis=1)\n",
    "        x -= tmp.reshape((x.shape[0], 1))\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis=1)\n",
    "        x /= tmp.reshape((x.shape[0], 1))\n",
    "    else:\n",
    "        # vector\n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "    return x\n",
    "\n",
    "def process(filename):\n",
    "    fin = open(filename, 'r', encoding='utf-8')\n",
    "    lines = fin.readlines()\n",
    "    fin.close()\n",
    "    idx2graph = {}\n",
    "    fout = open(filename+'.graph', 'wb')\n",
    "    for i in lines:\n",
    "        sentence = i.strip().split(\"\\t\")[0]\n",
    "        try:\n",
    "            adj_matrix = dependency_adj_matrix(sentence)\n",
    "        except:\n",
    "            print(filename)\n",
    "            raise\n",
    "        idx2graph[sentence] = adj_matrix\n",
    "    pickle.dump(idx2graph, fout)\n",
    "    fout.close()\n",
    "\n",
    "\n",
    "# filename = \"/home/qtxu/SPN/data/Camera-COQE/train_only_sentence.txt\"\n",
    "# process(filename)\n",
    "\n",
    "max_lens = 12\n",
    "sentence = \"Both iXUS 40 and 65 are Made in Japan . \"\n",
    "adj_matrix = dependency_adj_matrix(sentence)\n",
    "d_graph = np.pad(adj_matrix, ((1,max_lens-1-len(adj_matrix)), (1,max_lens-1-len(adj_matrix))), 'constant')\n",
    "d_graph\n",
    "type(d_graph)\n",
    "# import pdb\n",
    "# pdb.set_trace()\n",
    "# print(adj_matrix)\n",
    "# print(d_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"zh\" id=\"18ec656cffa7443482cdf5b484bad066-0\" class=\"displacy\" width=\"1800\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">钥匙</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">是</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">遥控</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">的</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">一体化</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">钥匙，</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">并</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">能</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">遥控</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">后备箱。</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-18ec656cffa7443482cdf5b484bad066-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,2.0 925.0,2.0 925.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-18ec656cffa7443482cdf5b484bad066-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-18ec656cffa7443482cdf5b484bad066-0-1\" stroke-width=\"2px\" d=\"M245,439.5 C245,89.5 920.0,89.5 920.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-18ec656cffa7443482cdf5b484bad066-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cop</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,441.5 L237,429.5 253,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-18ec656cffa7443482cdf5b484bad066-0-2\" stroke-width=\"2px\" d=\"M420,439.5 C420,177.0 915.0,177.0 915.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-18ec656cffa7443482cdf5b484bad066-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,441.5 L412,429.5 428,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-18ec656cffa7443482cdf5b484bad066-0-3\" stroke-width=\"2px\" d=\"M420,439.5 C420,352.0 555.0,352.0 555.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-18ec656cffa7443482cdf5b484bad066-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M555.0,441.5 L563.0,429.5 547.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-18ec656cffa7443482cdf5b484bad066-0-4\" stroke-width=\"2px\" d=\"M770,439.5 C770,352.0 905.0,352.0 905.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-18ec656cffa7443482cdf5b484bad066-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-18ec656cffa7443482cdf5b484bad066-0-5\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,264.5 1435.0,264.5 1435.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-18ec656cffa7443482cdf5b484bad066-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,441.5 L1112,429.5 1128,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-18ec656cffa7443482cdf5b484bad066-0-6\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,352.0 1430.0,352.0 1430.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-18ec656cffa7443482cdf5b484bad066-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux:modal</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,441.5 L1287,429.5 1303,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-18ec656cffa7443482cdf5b484bad066-0-7\" stroke-width=\"2px\" d=\"M945,439.5 C945,177.0 1440.0,177.0 1440.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-18ec656cffa7443482cdf5b484bad066-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1440.0,441.5 L1448.0,429.5 1432.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-18ec656cffa7443482cdf5b484bad066-0-8\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,352.0 1605.0,352.0 1605.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-18ec656cffa7443482cdf5b484bad066-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1605.0,441.5 L1613.0,429.5 1597.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    " \n",
    "nlp = spacy.load('zh_core_web_sm')\n",
    "# 如果是对中文的句子进行解析，则使用'zh_core_web_sm'\n",
    "# 注意无论是'en_core_web_sm'还是'zh_core_web_sm'的版本号都需要与spacy的版本保持一致\n",
    "doc = nlp(\"钥匙是遥控的一体化钥匙，并能遥控后备箱。\")\n",
    " \n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '油耗', 2: '比', 3: '骑车', 5: '要', 6: '高', 7: '。'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokens = ['油耗', '比', '骑车', '要', '高', '。']\n",
    "test_dict = {}\n",
    "count = 0\n",
    "for word in spacy_tokens:\n",
    "    test_dict[count] = word\n",
    "    count+=len(word)\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这', '是', '一', '段', '中', '文', '文', '本', '。']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'spacy.tokens.token.Token' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1637358/214587075.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# print(\"j is {}, token is {}\".format(j, token))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# print(token.children)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msingel_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0madj_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'spacy.tokens.token.Token' is not iterable"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "# 处理中文文本\n",
    "text = \"这是一段中文文本。\"\n",
    "from transformers import BertTokenizerFast\n",
    "import numpy as np\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"/home/qtxu/PLM/bert-base-chinese\")\n",
    "sen_tokens = tokenizer(text, add_special_tokens=False)\n",
    "sen_tokens_input_ids = sen_tokens['input_ids']\n",
    "sen_tokens_text_list = tokenizer.decode(sen_tokens['input_ids']).split(\" \")\n",
    "print(sen_tokens_text_list)\n",
    "doc = nlp(text)\n",
    "\n",
    "# 构造邻接矩阵\n",
    "num_tokens = len(doc)\n",
    "adjacency_matrix = np.zeros((len(sen_tokens_text_list), len(sen_tokens_text_list))).astype('float32')\n",
    "\n",
    "# 遍历文本中的所有单独的词\n",
    "for i, singel_token in enumerate(sen_tokens_text_list):\n",
    "    # print(\"i is {}, singel_token is {}\".format(i, singel_token))\n",
    "    adjacency_matrix[i][i] =1\n",
    "    for j, token in enumerate(doc):\n",
    "        for child in token.children:\n",
    "            adjacency_matrix[token.i][child.i] = 1\n",
    "\n",
    "    \n",
    "# for token in doc: # [这是 一 段 中文 文本]\n",
    "#     print(token)\n",
    "#     # 如果当前词有子节点，则在邻接矩阵中设置边\n",
    "#     for child in token.children:\n",
    "#         adjacency_matrix[token.i][child.i] = 1\n",
    "\n",
    "# print(adjacency_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[这是, 一, 段, 中文, 文本]\n",
      "i is 0, token is 这是\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1637358/724128289.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0madjacency_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# 保留自身属性特征\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# 如果当前词是句子的根节点，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdep_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ROOT'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('zh_core_web_sm')\n",
    "\n",
    "text = '这是一段中文文本'\n",
    "print(len(text)) # 中文文本的句子长度\n",
    "doc = nlp(text)\n",
    "\n",
    "# 获取文本中的所有词\n",
    "tokens = list(doc)\n",
    "print(tokens) #[这是, 一, 段, 中文, 文本]\n",
    "\n",
    "# 创建一个空的邻接矩阵\n",
    "adjacency_matrix = [[0 for _ in range(len(text))] for _ in range(len(text))]\n",
    "\n",
    "# 遍历所有词，填充邻接矩阵\n",
    "for i, token in enumerate(tokens):\n",
    "    print(\"i is {}, token is {}\".format(i,token))\n",
    "    adjacency_matrix[i][i] = 1 # 保留自身属性特征\n",
    "    # 如果当前词是句子的根节点，则跳过\n",
    "    stop()\n",
    "    if token.dep_ == 'ROOT':\n",
    "        continue\n",
    "    # 如果当前词的父节点是句子中的其他词，则在邻接矩阵中填入 1\n",
    "    if token.head.sent == token.sent:\n",
    "        adjacency_matrix[i][token.head.i] = 1\n",
    "        adjacency_matrix[token.head.i][i] = 1\n",
    "\n",
    "# 打印邻接矩阵\n",
    "print(adjacency_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', '-', '[UNK]', '/', '[UNK]', '以', '下', ',', '[UNK]', '驾', '驶', '乐', '趣', '远', '大', '于', '富', '康', ',', '座', '椅', '人', '机', '工', '程', '设', '计', '我', '认', '为', '极', '为', '出', '色', ',', '十', '分', '难', '能', '可', '贵', ',', '[UNK]', '就', '是', '为', '[UNK]', '(', '含', ')', '以', '下', '时', '速', '而', '设', '计', '的', ',', '德', '国', '人', '造', '车', '造', '神', '了', ',', '你', '不', '能', '不', '折', '服', ',', '我', '就', '是', '透', '过', '[UNK]', '和', '[UNK]', '认', '识', '德', '国', '车', '的', '。']\n",
      "90\n",
      "{0: '100', 1: '-', 2: '110', 3: 'K', 4: 'M', 5: '/', 6: 'H', 7: '以下', 9: ',', 10: 'SAIL', 11: '驾驶', 13: '乐趣', 15: '远', 16: '大于', 18: '富康', 20: ',', 21: '座椅', 23: '人机', 25: '工程', 27: '设计', 29: '我', 30: '认为', 32: '极为', 34: '出色', 36: ',', 37: '十分', 39: '难能可贵', 43: ',', 44: 'SAIL', 45: '就是', 47: '为', 48: '100', 49: 'K', 50: 'M(', 51: '含)', 53: '以下', 55: '时速', 57: '而', 58: '设计', 60: '的', 61: ',', 62: '德国人', 65: '造车', 67: '造神', 69: '了', 70: ',', 71: '你', 72: '不能不', 75: '折服', 77: ',', 78: '我', 79: '就是', 81: '透过', 83: 'JETTA', 84: '和', 85: 'SAIL', 86: '认识', 88: '德国', 90: '车', 91: '的', 92: '。'}\n",
      "['110', 'M', 'H', 'SAIL', 'SAIL', 'M(', 'JETTA', 'SAIL']\n",
      "100-#K#/#以下,#驾驶乐趣远大于富康,座椅人机工程设计我认为极为出色,十分难能可贵,#就是为100K#(含)以下时速而设计的,德国人造车造神了,你不能不折服,我就是透过#和#认识德国车的。\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def spacy_test_dict(text):\n",
    "    nlp = spacy.load(\"zh_core_web_sm\") # 加载spaCy模型\n",
    "    doc = nlp(text)\n",
    "    spacy_tokens = []\n",
    "    for token in doc:\n",
    "        spacy_tokens.append(token)\n",
    "    # 依据spacy的分词解析结果，存放开始的index\n",
    "    test_dict = {}\n",
    "    count = 0\n",
    "    for word in spacy_tokens:\n",
    "        if word.like_num: # 匹配纯数字的情况，长度为1\n",
    "            test_dict[count] = str(word)\n",
    "            count += 1  \n",
    "        elif re.findall(r\"[a-zA-Z]\", word.text): # 匹配纯字母的情况，长度为1\n",
    "            test_dict[count] = str(word)\n",
    "            count += 1\n",
    "        else: # 正常的情况\n",
    "            test_dict[count] = str(word)\n",
    "            count+=len(word)\n",
    "    return test_dict\n",
    "\n",
    "def get_split(spacy_results, bert_results):\n",
    "    not_complete_in_bert_results = {key:value for key, value in spacy_results.items() if \n",
    "    value not in bert_results and value not in ''.join(bert_results)} # 获取在bert中完全未出现的词语\n",
    "    concatenated_values = []\n",
    "    current_list = []\n",
    "\n",
    "    previous_key = None\n",
    "    for key in sorted(not_complete_in_bert_results.keys()):\n",
    "        if previous_key is not None and key == previous_key + 1:\n",
    "            current_list.append(not_complete_in_bert_results[key])\n",
    "        else:\n",
    "            if current_list:\n",
    "                concatenated_values.append(''.join(current_list))\n",
    "            current_list = [not_complete_in_bert_results[key]]\n",
    "        previous_key = key\n",
    "\n",
    "    if current_list:\n",
    "        concatenated_values.append(''.join(current_list))\n",
    "    return concatenated_values\n",
    "\n",
    "sentence = \"100-110KM/H以下,SAIL驾驶乐趣远大于富康,座椅人机工程设计我认为极为出色,十分难能可贵,SAIL就是为100KM(含)以下时速而设计的,德国人造车造神了,你不能不折服,我就是透过JETTA和SAIL认识德国车的。\"\n",
    "bert_path = \"/home/qtxu/PLM/bert-base-chinese\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(bert_path)\n",
    "tokenized = tokenizer.tokenize(sentence)\n",
    "print(tokenized)\n",
    "print(len(tokenized))\n",
    "\n",
    "spacy_test_dict = spacy_test_dict(sentence) # 获取spacy解析的结果\n",
    "print(spacy_test_dict)\n",
    "list_result = get_split(spacy_test_dict, tokenized) # 获取bert tokenized 结果与spacy解析结果相比，spacy解析结果完全不在bert tokenized中的结果\n",
    "\n",
    "print(list_result)\n",
    "for i in range(len(list_result)): # 将合并之后的单词，用#替换\n",
    "    sentence = sentence.replace(list_result[i], \"#\")\n",
    "    \n",
    "print(sentence) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'longer'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_split(spacy_results, bert_results):\n",
    "    not_complete_in_bert_results = {key:value for key, value in spacy_results.items() if \n",
    "    value not in bert_results and value not in ''.join(bert_results)} # 获取在bert中完全未出现的词语\n",
    "    concatenated_values = []\n",
    "    current_list = []\n",
    "\n",
    "    previous_key = None\n",
    "    for key in sorted(not_complete_in_bert_results.keys()):\n",
    "        if previous_key is not None and key == previous_key + 1:\n",
    "            current_list.append(not_complete_in_bert_results[key])\n",
    "        else:\n",
    "            if current_list:\n",
    "                concatenated_values.append(''.join(current_list))\n",
    "            current_list = [not_complete_in_bert_results[key]]\n",
    "        previous_key = key\n",
    "\n",
    "    if current_list:\n",
    "        concatenated_values.append(''.join(current_list))\n",
    "    return concatenated_values\n",
    "\n",
    "sent_tokened = ['感', '觉', '比', '[UNK]', '的', '抖', '动', '好', '得', '多', ',', '而', '且', '空', '调', '降', '温', '比', '[UNK]', '得', '还', '好', '.']\n",
    "test_dict = {'感觉-0': '好得多-6', '比-1': '抖动-5', 'TJ-2': '抖动-5', '376-3': '抖动-5', '的-4': '376-3', '抖动-5': '好得多-6', '好得多-6': 'Root', ',-7': '好得多-6', '而且-8': '好得多-6', '空调-9': '降温-10', '降温-10': '而且-8', '比-11': '还好-15', 'LJ-12': '还好-15', '465-13': '还好-15', '得-14': '还好-15', '还好-15': '降温-10', '.-16': '还好-15'}\n",
    "get_split(sent_tokened, test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list1 = ['感觉', '比', 'TJ', '376', '的', '抖动', '好得多', ',', '而且', '空调', '降温', '比', 'LJ', '465', '得', '还好', '.']\n",
    "import re\n",
    "def count_len(stanford_words):\n",
    "    sum_len = 0\n",
    "    for i in range(len(list1)):\n",
    "        if re.findall(r\"\\d+[a-zA-Z]+[\\u4e00-\\u9fff]+|\\d|[a-zA-Z]|\\d+[a-zA-Z]+[\\u4e00-\\u9fff]+|[\\u4e00-\\u9fff]+\\d\", list1[i]):\n",
    "            sum_len += 1\n",
    "        else:\n",
    "            sum_len += len(list1[i])\n",
    "    return sum_len\n",
    "\n",
    "count_len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TJ376', 'LJ465']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dependency_adj_matrix_zh(sen, sent_tokened):\n",
    "    stanford_words = list(zh_model.word_tokenize(sen))\n",
    "    stan_dict = get_stan_dict(sen)\n",
    "    test_dict = {}\n",
    "    count = 0\n",
    "    for word in stanford_words:\n",
    "        if word.isdigit():\n",
    "            test_dict[count] = word\n",
    "            count += 1\n",
    "        elif word == \"DX\":\n",
    "            test_dict[count] = word\n",
    "            count += 1  \n",
    "        elif len(re.findall(r\"\\d+[a-zA-Z]+[\\u4e00-\\u9fff]+\", word)) != 0: # 匹配数字字母汉字格式的str, eg 4s站\n",
    "            test_dict[count] = word\n",
    "            count += 1\n",
    "        elif len(re.findall(r\"\\d+[\\u4e00-\\u9fff]+\", word)) != 0: # 匹配数字汉字格式， eg:呈V\n",
    "            test_dict[count] = word\n",
    "            count +=1\n",
    "        elif len(re.findall(r\"[a-zA-Z]+\\d+|\\d+[.,-]+\\d+\", word)) != 0:  # 匹配字母数字格式，或数字标点符号数字格式 eg:5W， 25.4 or 11-13\n",
    "            test_dict[count] = word\n",
    "            count += 1\n",
    "        elif len(re.findall(r\"[\\u4e00-\\u9fff]+\\d\", word)) != 0: #匹配汉字数字格式，eg:\n",
    "            test_dict[count] = word\n",
    "            count += 1   \n",
    "        elif re.findall(r\"[a-zA-Z]\", word):\n",
    "            test_dict[count] = word\n",
    "            count += 1         \n",
    "        else: # 正常的情况\n",
    "            test_dict[count] = word\n",
    "            count+=len(word)\n",
    "    # print(\"test_dict\", test_dict)\n",
    "\n",
    "    # number = 0\n",
    "    adj_matrix = np.eye(len(sent_tokened)).astype('float32') # sent_tokened:表示的是经过berttokenfast分词之后的结果\n",
    "    for key_i, value_i in stan_dict.items():\n",
    "        try:\n",
    "            key_word, key_id = key_i.rsplit(\"-\", 1)\n",
    "        except:\n",
    "            print(key_i)\n",
    "            # stop()\n",
    "        try:\n",
    "            value_word, value_id = value_i.rsplit('-', 1) # 不同于split(\"-\"), 其可以处理11-13-13这样的特殊情况\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        key_start = next(key for key, value in test_dict.items() if value == key_word)\n",
    "        value_start = next(key for key, value in test_dict.items() if value == value_word)\n",
    "\n",
    "        for i in range(key_start, key_start + len(key_word)): # 注意此处的id是test_dict中的id,而不是stan_dict中的id\n",
    "            for j in range(value_start, value_start + len(value_word)):\n",
    "                adj_matrix[i][j] = 1\n",
    "                adj_matrix[j][i] = 1\n",
    "\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' dvd和#刻录方面，#支持#刻录和#刻录，与#类似，#刻录速度降#。'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "diff_results = ['Cd', 'SHB022a', '12Xdvdr', '#', 'SEB026a', 'Cd', '32X']\n",
    "sen_new = \" dvd和Cd刻录方面，SHB022a支持12Xdvdr刻录和#刻录，与SEB026a类似，Cd刻录速度降32X。\"\n",
    "for diff in range(len(diff_results)):\n",
    "    sen_new = sen_new.replace(diff_results[diff], \"#\") # 将解析的不同词语使用特殊符号#替换， 对应bert解析的unk\n",
    "\n",
    "sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "# file_name = os.path.join(args.data_path, f'{dep_mode_path}_sentences_labels.txt.graph') \n",
    "# file_name = \"/home/qtxu/Sentiment-SPN/data/Car-COQE/train_sentences_labels.txt.graph\"\n",
    "file_name = \"/home/qtxu/Sentiment-SPN/data/Car-COQE/train_test.txt.graph\"\n",
    "file_size = os.stat(file_name).st_size\n",
    "if file_size == 0: # 增加一个判断条件，判断是否为空\n",
    "    raise Exception(\"Error:File is empty.\")\n",
    "else:\n",
    "    fgraph = open(file_name, \"rb\") # \"rb\"是文件打开模式。\"r\"表示读取模式，\"b\"表示二进制模式。\n",
    "    idx2graph = pickle.load(fgraph)\n",
    "    fgraph.close() # very import, sure added\n",
    "\n",
    "text = \"但坐进车内感觉又截然不同，似乎不是一个年代的车型，福美来显得很现代、很舒适，色调的搭配还营造出一种家庭的温馨，与菱帅的内饰水准明显不在一个档次上。　　\"\n",
    "# text = \"与原来1.6宝来相比，还算可以接受。\"\n",
    "dependency_adj_matrix = idx2graph[text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#跑车的动力系统也较#更加强劲。\n",
      "#跑车的动力系统也较#更加强劲。\n"
     ]
    }
   ],
   "source": [
    "diff_results = ['207CC', '206CC']\n",
    "# sen = \"现在出产的幸福使者跟以前已经大不相同了，不能说装配和工艺上已经无懈可击，但绝对能够达到夏利以上的水平，不信你随便找出一辆型号为7100A的一汽佳星#幸福使者和一辆夏利打开发动机盖看一下，我可以肯定的说幸福使者的装配明显比夏利高一个档次。\"\n",
    "# sen = \"207CC跑车的动力系统也较206CC更加强劲。\"\n",
    "sen = \"但坐进车内感觉又截然不同，似乎不是一个年代的车型，福美来显得很现代、很舒适，色调的搭配还营造出一种家庭的温馨，与菱帅的内饰水准明显不在一个档次上。\"\n",
    "sen = \" 款马6在配置方面拥有比新马6更实在一些。\"\n",
    "for diff in range(len(diff_results)):\n",
    "    sen = sen.replace(diff_results[diff], \"#\") # 将解析的不同词语使用特殊符号#替换， 对应bert解析的unk\n",
    "sen_new = sen\n",
    "print(sen)\n",
    "print(sen_new)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtxu_env_3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0 (default, Oct  9 2018, 10:31:47) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a47ee85e4d3fd43d766ef318c39ba9ff51af2e172aae0c1beade709e776e5b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
