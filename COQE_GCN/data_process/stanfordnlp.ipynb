{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mFailed to start the Kernel 'qtxu_env_3.7 (Python 3.7.0)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. EROFS: read-only file system, open '/tmp/kernel-v2-13294887eAQa8hL0y1F.json'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "model_path = \"/home/qtxu/PLM/stanford-corenlp-full-2018-02-27\"\n",
    "zh_model = StanfordCoreNLP(model_path, lang='zh', quiet=True, logging_level=logging.DEBUG)\n",
    "\n",
    "sen = \"这款车的发动机异于同类。\"\n",
    "words = list(zh_model.word_tokenize(sen))\n",
    "print(words)\n",
    "print(\"该句子解析后的长度是：\",len(words))\n",
    "arcs = list(zh_model.dependency_parse(sen))\n",
    "print(arcs) # include Root\n",
    "\n",
    "arcs.sort(key=lambda x: x[2]) # 按照在sentence中出现的次序排序\n",
    "print(arcs)\n",
    "words = [w + \"-\" + str(idx) for idx, w in enumerate(words)]\n",
    "rely_id = [arc[1] for arc in arcs]\n",
    "print(\"rely_id\", rely_id)\n",
    "relation = [arc[0] for arc in arcs]\n",
    "print(\"relation\", relation)\n",
    "# heads = ['Root' if id == 0 else words[id - 1] for id in rely_id]\n",
    "heads = ['Root' if id == 0 else words[id - 1] for id in rely_id]\n",
    "matrix1 = np.zeros((len(words), len(words))).astype('float32')\n",
    "\n",
    "def dependency_adj_matrix_zh(sen, sent_tokened):\n",
    "    stanford_words = list(zh_model.word_tokenize(sen))\n",
    "    # sen_tokens_text_list = tokenizer.tokenize(text)\n",
    "    # nlp = spacy.load(\"zh_core_web_sm\") # 加载spaCy模型\n",
    "    # doc = nlp(text) # 解析文本\n",
    "\n",
    "    # spacy_tokens = []\n",
    "    # for token in doc:\n",
    "    #     spacy_tokens.append(token) # ['油耗', '比', '骑车', '要', '高', '。']\n",
    "    # print(spacy_tokens)\n",
    "\n",
    "    # 依据spacy的分词解析结果，存放开始的index\n",
    "    test_dict = {}\n",
    "    count = 0\n",
    "    for word in stanford_words:\n",
    "        # print(type(word.text))\n",
    "        if word.like_num:\n",
    "            test_dict[count] = word\n",
    "            count += 1\n",
    "        elif word.text == \"DX\":\n",
    "            test_dict[count] = word\n",
    "            count += 1  \n",
    "        elif len(re.findall(r\"\\d+[a-zA-Z]+[\\u4e00-\\u9fff]+\", word.text)) != 0: # 匹配数字字母汉字格式的str, eg 4s站\n",
    "            test_dict[count] = word\n",
    "            count += 1\n",
    "        elif len(re.findall(r\"\\d+[\\u4e00-\\u9fff]+\", word.text)) != 0: # 匹配数字汉字格式， eg:\n",
    "            test_dict[count] = word\n",
    "            count +=1\n",
    "        elif len(re.findall(r\"[a-zA-Z]+\\d+\", word.text)) != 0:  # 匹配字母数字格式， eg:5W\n",
    "            test_dict[count] = word\n",
    "            count += 1\n",
    "        elif len(re.findall(r\"[\\u4e00-\\u9fff]+\\d\", word.text)) != 0: #匹配汉字数字格式，eg:\n",
    "            test_dict[count] = word\n",
    "            count += 1   \n",
    "        elif re.findall(r\"[a-zA-Z]\", word.text):\n",
    "            test_dict[count] = word\n",
    "            count += 1         \n",
    "        else: # 正常的情况\n",
    "            test_dict[count] = word\n",
    "            count+=len(word)\n",
    "    # print(\"spacy results:\", test_dict)\n",
    "    # print(test_dict) # {0: '油耗', 2: '比', 3: '骑车', 5: '要', 6: '高', 7: '。'}\n",
    "\n",
    "    number = 0\n",
    "    adj_matrix = np.eye(len(sent_tokened)) # sent_tokened:表示的是经过berttokenfast分词之后的结果\n",
    "    i = 0\n",
    "    while i < len(sent_tokened):\n",
    "        word = stanford_words[number]# 获取第number个stanford解析的结果\n",
    "\n",
    "        word_sp = list(word.text)\n",
    "        for child in word.children:\n",
    "            adj_word_list = list(child.text) # 具有依存关系的词语\n",
    "            word_list = list(word.text) # spacy分词的结果中的某一个词\n",
    "            child_key = next(key for key, val in test_dict.items() if val == child) # obtain the start index of child\n",
    "            word_key = next(key for key, val in test_dict.items() if val == word) # obtain the start index of spacy_word\n",
    "            # print(\"child:{}, word:{}\".format(child, word))\n",
    "            for m in range(child_key, len(adj_word_list) + child_key):\n",
    "                for n in range(word_key, len(word_list) + word_key):\n",
    "                    try:\n",
    "                        adj_matrix[m][n] = 1\n",
    "                        adj_matrix[n][m] = 1\n",
    "                    except:\n",
    "                        print(text)\n",
    "                        # stop()\n",
    "        i += len(word_sp)\n",
    "        number += 1\n",
    "    return adj_matrix\n",
    "\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print(relation[i] + '(' + words[i] + ', ' + heads[i] + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mFailed to start the Kernel 'qtxu_env_3.7 (Python 3.7.0)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. EROFS: read-only file system, open '/tmp/kernel-v2-3305004bQQv2szM07fg.json'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "model_path = \"/home/qtxu/PLM/stanford-corenlp-full-2018-02-27\"\n",
    "zh_model = StanfordCoreNLP(model_path, lang='zh', quiet=True, logging_level=logging.DEBUG)\n",
    "\n",
    "sen = \"这款车的发动机异于同类。\"\n",
    "words = list(zh_model.word_tokenize(sen))\n",
    "print(words)\n",
    "print(\"该句子解析后的长度是：\",len(words))\n",
    "arcs = list(zh_model.dependency_parse(sen))\n",
    "print(arcs) # include Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict={0: '这', 1: '款', 2: '车', 3: '的', 4: '发动机', 7: '。'}\n",
    "stan_dict={'这-0': '车-2', '款-1': '这-0', '车-2': '发动机-4', '的-3': '车-2', '发动机-4': 'Root', '。-5': '发动机-4'}\n",
    "for key_i, value_i in stan_dict.items():\n",
    "    # print(\"stan_dict\", stan_dict)\n",
    "    key_word, key_id = key_i.split(\"-\")\n",
    "    try:\n",
    "        value_word, value_id = value_i.split(\"-\")\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    key_start = next(key for key, value in test_dict.items() if value == key_word)\n",
    "    value_start = next(key for key, value in test_dict.items() if value == value_word)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtxu_env_3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0 (default, Oct  9 2018, 10:31:47) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a47ee85e4d3fd43d766ef318c39ba9ff51af2e172aae0c1beade709e776e5b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
